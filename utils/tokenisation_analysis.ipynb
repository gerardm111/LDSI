{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.attrs import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Veterans Law Judge', [{ORTH: 'Veterans Law Judge'}])\n",
    "nlp.tokenizer.add_special_case('Veterans Affairs', [{ORTH: 'Veterans Affairs'}])\n",
    "nlp.tokenizer.add_special_case(\"Veterans' Appeals\", [{ORTH: \"Veterans' Appeals\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "patterns = [[{\"TEXT\": \"[\"}], [{\"TEXT\": \"\\n\"}], [{\"TEXT\": \"'\"}], [{\"TEXT\": \"\\r\"}], [{\"TEXT\": \"t\"}]]\n",
    "attrs = {\"POS\": \"PUNCT\"}\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=0)\n",
    "\n",
    "nlp.disable_pipes('parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(txt):\n",
    "    #doc = nlp(txt)\n",
    "    clean_tokens = {}\n",
    "    docs = nlp.pipe(txt, n_process=4)\n",
    "    cpt=0\n",
    "    for doc in docs:\n",
    "        clean_tokens[cpt]=[]\n",
    "        tokens = list(doc)\n",
    "        par_removed = ''\n",
    "        for t in tokens:\n",
    "            if t.pos_ == 'PUNCT':\n",
    "                pass\n",
    "            elif t.pos_ == 'NUM':\n",
    "                clean_tokens[cpt].append(f'<NUM{len(t)}>')\n",
    "            elif t.lemma_ == \"'s\":\n",
    "                pass\n",
    "            elif '(' in t.lemma_:\n",
    "                par_split = t.lemma_.split('(')\n",
    "                for elem in par_split:\n",
    "                    par_removed = par_removed + elem\n",
    "                par_split = spacy_tokenize([par_removed])\n",
    "                for elem in par_split:\n",
    "                    clean_tokens[cpt].append(elem)\n",
    "            elif \"\\n\" in t.lemma_:\n",
    "                par_split = t.lemma_.split('\\n')\n",
    "                for elem in par_split:\n",
    "                    if elem != ' ' and elem != '':\n",
    "                        par_removed = par_removed + ' ' + elem\n",
    "                par_split = spacy_tokenize(par_removed)\n",
    "                for elem in par_split:\n",
    "                    clean_tokens[cpt].append(elem)\n",
    "            else:\n",
    "                clean_tokens[cpt].append(t.lemma_.lower())\n",
    "        cpt += 1\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_procedure_1 = \"Archive Date: 07/11/16\"\n",
    "example_short_1 = \"He related that he was\"\n",
    "example_fact_1 = \"He related that he was \\\"having a reoccurrence of bronchitis [symptoms].\\\"\"\n",
    "example_procedure_2 = \"Veterans Law Judge, Board of Veterans' Appeals\\n\\n\\n\\nDepartment of Veterans Affairs\"\n",
    "example_cit_2 = \"38 U.S.C.A. \\u00a7\\u00a7 1101, 1110 (West 2014); 38 C.F.R. \\u00a7\\u00a7 3.303, 3.304 (2015).\"\n",
    "example_seg_1 = \"Acting Veterans \\n Law Judge, Board of Veterans' Appeals\\n\\nUnder 38 U.S.C.A. \\u00a7 7252, only \\ntoday a decision of the Board is appealable to the Court.\"\n",
    "example_basic_1 = 'In sum, as the preponderance of the evidence is against the Veteran\\'s claim, his appeal must be denied.'\n",
    "example_cit_1 = 'Smith v. Gober, 14 Vet. App. 227 (2000), aff\\'d 281 F.3d 1384 (Fed. Cir. 2002); Dela Cruz v. Principi, 15 Vet. App. 143 (2001); see also Quartuccio v. Principi, 16 Vet. App. 183 (2002).'\n",
    "example_rule_1 = '\"To establish a right to compensation for a present disability, a Veteran must show: \"(1) the existence of a present disability; (2) in-service incurrence or aggravation of a disease or injury; and (3) a causal relationship between the present disability and the disease or injury incurred or aggravated during service\"-the so-called \"nexus\" requirement.\"'\n",
    "example_mixed_1 = 'In Dingess v. Nicholson, 19 Vet. App. 473 (2006), the U.S. Court of Appeals for Veterans Claims held that, upon receipt of an application for a service-connection claim, 38 U.S.C.A. � 5103(a) and 38 C.F.R. � 3.159(b) require VA to provide the claimant with notice that a disability rating and an effective date for the award of benefits will be assigned if service connection is awarded. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [example_seg_1, example_cit_1]\n",
    "text2 = [example_short_1, example_basic_1]\n",
    "#print(text)\n",
    "dict_test1 = spacy_tokenize(text)\n",
    "dict_test2 = spacy_tokenize(text2)\n",
    "#print(dict_test, len(dict_test))\n",
    "corpus_test = {\"24.txt\": dict_test1, \"5678.txt\": dict_test2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly selecting sentence order and written them in file\n",
    "Step 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "while (len(corpus_test)) != 0:\n",
    "    #choose randomly a doc\n",
    "    random_key = choice(list(corpus_test))\n",
    "    # choose randomly a sentence\n",
    "    random_sent = choice(list(corpus_test[random_key]))\n",
    "    # remove the sentence\n",
    "    temp = corpus_test[random_key].pop(random_sent)\n",
    "    ## write this sentence if nb_tokens >= 5 in txt file with white spaces between each tokens\n",
    "    if len(temp) >= 5:\n",
    "        with open('test_tokens.txt', 'a+') as file:\n",
    "            for elem in temp:\n",
    "                file.write(str(elem) + ' ')\n",
    "            file.write('\\n')\n",
    "    ## if last sentence of doc remove doc\n",
    "    if len(corpus_test[random_key]) == 0:\n",
    "        del corpus_test[random_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
